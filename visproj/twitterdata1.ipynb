{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "consumer_key = \"U6eIfx0myrDZxymjY6rxFQuH3\"\n",
    "consumer_secret = \"bmHWK7d1ktxIa43iOar1eyPs2kG9frLFecv0E6mdYWXex3VCnM\"\n",
    "access_token = \"1351114116291842050-Sqx8kbMr7mVneQasWdjkrj9CH4NfI8\"\n",
    "access_token_secret = \"VfmTCgDjmPRAQ7vwGMysI6izKnvgPSrRqqO035ZPxDIPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['created_at', 'id', 'id_str', 'text', 'truncated', 'entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'lang'])\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.keys())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get first tweets from topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlp = api.search(\"abb\", count = 10, lang = \"sv\")\n",
    "len(xlp[\"statuses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kunal Sharma : @ayeajanabi Kiska kata abb?\n",
      "################\n",
      "Kunal Sharma\n",
      "Kars : @BengtHojer Bist√•nd √§r mutor, kr√§vs motprestation som √§r att k√∂pa, kraftf√∂rs√∂rjning ABB och tele infrastruktur Eric‚Ä¶ https://t.co/rPL99sAOXk\n",
      "################\n",
      "Kars\n",
      "Harsh (shehnaazGill meri sweetheart) ‚ù§üòç : RT @Mithoo1290: Ignore mat karna abb ü•∫üôè https://t.co/OGWvf0ebt5\n",
      "################\n",
      "Harsh (shehnaazGill meri sweetheart) ‚ù§üòç\n",
      "Dr Anvesa Chaudhary : RT @Mithoo1290: Ignore mat karna abb ü•∫üôè https://t.co/OGWvf0ebt5\n",
      "################\n",
      "Dr Anvesa Chaudhary\n",
      "ùêåùê¢ùê≠ùê°ùê®ùê®üñ§ü•Ä|| Shehnaaz ‚ô° : Ignore mat karna abb ü•∫üôè https://t.co/OGWvf0ebt5\n",
      "################\n",
      "ùêåùê¢ùê≠ùê°ùê®ùê®üñ§ü•Ä|| Shehnaaz ‚ô°\n",
      "Jotüíï : Ignore mat karna abb üôè https://t.co/msJLRtyWLs\n",
      "################\n",
      "Jotüíï\n",
      "Pranvi : RT @Nelson_04_: @SafeedBandarya Ohh big fan abb tweet rt karr\n",
      "\n",
      "FOREVER WITH RKV\n",
      "################\n",
      "Pranvi\n",
      "Sandesh Uparkoti : RT @Nelson_04_: @SafeedBandarya Ohh big fan abb tweet rt karr\n",
      "\n",
      "FOREVER WITH RKV\n",
      "################\n",
      "Sandesh Uparkoti\n",
      "Priyp : RT @Nelson_04_: @SafeedBandarya Ohh big fan abb tweet rt karr\n",
      "\n",
      "FOREVER WITH RKV\n",
      "################\n",
      "Priyp\n",
      "Safeed Bandariyaüôà(Sanskari trollerüòèüòé) : RT @Nelson_04_: @SafeedBandarya Ohh big fan abb tweet rt karr\n",
      "\n",
      "FOREVER WITH RKV\n",
      "################\n",
      "Safeed Bandariyaüôà(Sanskari trollerüòèüòé)\n"
     ]
    }
   ],
   "source": [
    "tweets = list()\n",
    "for tweet in xlp[\"statuses\"]:\n",
    "    print(tweet.get(\"user\").get(\"name\"), \":\", tweet.get(\"text\"))\n",
    "    tweets.append(tweet.get(\"user\").get(\"name\"), \":\", tweet.get(\"text\"))\n",
    "    print(\"################\")\n",
    "     \n",
    "    #author = np.array(tweet.get(\"user\").get(\"name\"));\n",
    "    #print(author)\n",
    "    \n",
    "    #tweets see that it put in a list\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up data and from companies and place in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name                                               Text\n",
      "0  Miranda  RT @SvenskHora: ‚ÄùEn kvinna fr√•n Europas fattig...\n",
      "                                    Name  \\\n",
      "0  Real World Photographs International.   \n",
      "\n",
      "                                                Text  \n",
      "0  People wait for food at a falafel shop in Malm...  \n",
      "        Name                                               Text\n",
      "0  faktoider  Det s√§gs att man i Turkmenistan f√∂rbjudit svar...\n",
      "                        Name  \\\n",
      "0  Birger p√• Jarlsgatan, üê∑üêñüêÄ   \n",
      "\n",
      "                                                Text  \n",
      "0  @janlenander @LundblomMy @liberalerna Jans cit...  \n",
      "     Name                                               Text\n",
      "0  POGOSJ  Milpitas: Bunnelby 97.8% (14/15/15) CP:328 (L2...\n",
      "               Name                                               Text\n",
      "0  Stefan Jovanoviƒá  @carerrra @veolenes Sjukt hur ni till h√∂ger al...\n",
      "         Name                                               Text\n",
      "0  Majsovitch  @PontusPersson5 En s√•n h√§r knapp borde de ha \\...\n",
      "   Name                                               Text\n",
      "0  Olle  @gma_z1 @Rackhamred1 @sueonum Du borde testa e...\n",
      "           Name                                               Text\n",
      "0  Iren S Cilla  @erikssondan @JennyNilssonGV Du kan studera An...\n",
      "       Name                                               Text\n",
      "0  Kenikeni  ‚ù§Ô∏è  Peg och Pog: Svenska üá∏üá™  Interaktiv pekbok...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_data(my_tweets):  #still alot to fic here\n",
    "    tweet_list = []\n",
    "    for i in my_tweets:      \n",
    "\n",
    "        proc_tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                        '(?:%[0-9a-fA-F][0-9a-fA-F]))+','',i)\n",
    "\n",
    "        proc_tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", proc_tweet)\n",
    "        print(i)\n",
    "        proc_tweet = proc_tweet.lower()\n",
    "   \n",
    "        proc_tweet = re.sub(r'\\W', ' ', proc_tweet)\n",
    "            \n",
    "        proc_tweet = re.sub(r'\\s+', ' ', proc_tweet, flags=re.I)\n",
    "\n",
    "        proc_tweet = proc_tweet.replace('rt','')\n",
    "\n",
    "        proc_tweet = re.findall(r'\\w+', proc_tweet) \n",
    "        \n",
    "        tweet_list.append(proc_tweet)\n",
    "    \n",
    "    return tweet_list\n",
    "\n",
    "#tweet.get(\"user\").get(\"name\"), \":\", don't work yet\n",
    "\n",
    "\n",
    "\n",
    "for tweet in xlp[\"statuses\"]:\n",
    "        data = {'Name': [tweet.get(\"user\").get(\"name\")] ,\n",
    "                'Text': [tweet.get(\"text\")] } \n",
    "              \n",
    "        df = pd.DataFrame(data, columns= ['Name', 'Text']) \n",
    "        #df = clean_data(df)\n",
    "        df.to_csv (r'C:\\Users\\Gustav\\Documents\\Lab1-Interactive_visualization\\infovisproj\\visproj\\data\\test.csv', index = False, header=True)\n",
    "        print (df)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply k-means or other appropriate clustering alogrithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store in CSV files or other for javascript to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-198-0bf1a510a1bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclean_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "clean_tweets = list()\n",
    "for tweet in tweets:   \n",
    "    doc = nlp(tweet)\n",
    "    clean_tweet = \"\"\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            clean_tweet += token.text + \" \"\n",
    "    print(clean_tweet)\n",
    "    clean_tweets.append(clean_tweet)\n",
    "    print(\"-------------\")\n",
    "    \n",
    "print(clean_tweets)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Store in csv files or similar  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
